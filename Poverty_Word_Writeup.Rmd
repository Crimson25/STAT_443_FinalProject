---
title: "Final Project"
author: "Kiera Grant, Braden Barglind, and Colin Fitzpatrick"
date: "12/4/2020"
output:
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
The objective of this study is to determine an appropriate logistic regression model in order to classify individuals into one of four poverty levels. Many social programs have a hard time making sure the right people are given enough aid. Often times the poorest of the population might not be given enough aid because they typically cannot provide the necessary income and expense records to prove that they qualify. Instead, this study will consider a family’s observable household attributes such as the material of their walls and ceiling, or the assets found in the home, to classify poverty levels. The original dataset contains survey responses from 9,057 individuals with 142 predictor variables. For purposes of this study, only data from the "heads of household" will be used which focuses the data down to 2,819 observations. 

In order to find a sufficient model, all of the predictor variables will be assessed for repetitiveness and skewness and potentially transformed. Relationships between predictor variables and poverty level as well as between the different predictor variables will need to be evaluated. This will help determine any preliminary multicollinearity issues so that these variables can be removed from the dataset early. The data will be split into a training data set as well as a validation data set in order to test the accuracy of our models. An initial multinomial logistic regression model will be created using the training data. The model will then be used to predict the poverty level of samples to determine the effectiveness of the model. Model building and model selection tools will be used to determine if more accuracy is gained through a new model. In addition, the micro-average value will be computed for each model built in order to assess the weighted accuracy of these multinomial models. The final model will be determined based off of these analyses in hopes to have an appropriate model to classify individuals into a poverty class. 

## Exploring and Transforming the Data
The first thing that we had to take care of was limiting the amount of variables to a sizable amount.  We then deleted all redundant variables which measured the same things but called by different names.  An example of this is these redundant variables hhsize and tamhog, which both measure the size of the household.  So we are able to get rid of one of them and keep the other.  Another thing we did with the data is to combine some of the terms so that we would have one variable that was now a binomial response. An example of this is, there was a male and female tab, which we are able to remove one of them because it gives redundant information.  Another way we partioned through the data was to get rid of variables based in the percentage of reponses that they had.  For example, one of the identifiers was if the house had a bathroom or not and the probability of houses having a bathroom was .995.  This results in a bad predictor as too many people have a bathroom so it is not a good predictor in that since.  So after those three ideas took place we were left with 34 variables left out of 142.
```{r message=FALSE, warning=FALSE, echo=FALSE, include=FALSE}
library(tidyverse) 
library(ggrepel) 
library(kableExtra) 
library(RColorBrewer) 
library(corrplot) 
library(caret) 
library(e1071) 
library(dominanceanalysis) 
library(readxl)

project_data <-  read_excel("HeadOfHouseHoldFile.xlsx")
head(project_data)

project_data2 <- read_excel("HeadOfHouseHoldFileUpdate.xlsx")
head(project_data2)

project_data_final <- read_excel("FinalChanges.xlsx")
head(project_data_final)

##CLEAN UP DATA
#replaces one variable with the variable that was transformed
project_data_final$tamviv <- project_data_final$tamvivsqrt
names(project_data_final)[3] <- "tamviv_sqrt"

#removes reference variables (and removes copy of transformed variable)
project_data_final <- project_data_final %>% dplyr::select(-c("paredblolad","paredpreb", "energcocinar2", "energcocinar3", "Married/Free Coupled", "div,sep,wid", "single", "parentesco1","tamvivsqrt"))

project_data_final$Target <- as.factor(project_data_final$Target) # sets Target to factor for analyses
apply(is.na(project_data_final), 2, which) #detects NAs in data frame, 3 values in meaneduc (1287, 1400, 1455), 1 value in depend (1287)
project_data_final[is.na(project_data_final)]=0  #replaces NAs with 0, can replace with zero for these categories


##EXPLORING THE DATA - QUANTITATIVE
#Rooms Plot (normal)
roomcounts <- table(project_data$rooms)
barplot(roomcounts,main="Number of Rooms", xlab="Room Counts")

#Number of Tablets Plot (heavy skew right)
tabletscounts <- table(project_data$v18q1)
barplot(tabletscounts,main="Number of Tablets", xlab="Tablet Counts")

#Number of People in Household Plot (skew right)
peoplecounts <- table(project_data$tamviv)
barplot(peoplecounts, main="Number of People in Household", xlab="People Counts")

#Children 0-19 Plot (skew right)
childrencounts <- table(project_data$hogar_nin)
barplot(childrencounts, main="Number of Children 0-19", xlab="Children Counts")

#Number of Adults Plot (small skew right)
adultcounts <- table(project_data$hogar_adul)
barplot(adultcounts, main="Number of Adults", xlab="Adult Counts")

#People 65+ Plot (heavy skew right)
people65counts <- table(project_data$hogar_mayor)
barplot(people65counts, main="People 65+", xlab = "People Counts")

#EdJefe Plot (weird)
edjefecounts <- table(project_data$edjefe)
barplot(edjefecounts, main="Ed Jefe Counts", xlab = "Counts")

#EdJefa Plot (weird)
edjefacounts <- table(project_data$edjefa)
barplot(edjefacounts, main="Ed Jefa Counts", xlab="Counts")

#Mean Education Plot (weird)
meaneducounts <- table(project_data$meaneduc)
barplot(meaneducounts, main="Mean Education", xlab="Counts")

#Overcrowding Plot (weird)
overcrowdcounts <- table(project_data$overcrowding)
barplot(overcrowdcounts, main="Overcrowding", xlab="Counts")

#Number of Dependents (weird)
dependcounts <- table(project_data$depend)
barplot(dependcounts, main="Dependencies", xlab="Counts")


##TRANSFORMATIONS
#Number of people in Household transformation: Square root  (only transformation that showed significant change)
sqrttamviv <- (sqrt(project_data$tamviv))
hist(sqrttamviv)


##EXPLORING RELATIONSHIPS

#Relationships between predictors
quantvars <- c(1,2,3,13,14,15,16,17,18,19,22,25)
categorvars <- c(4,5,6,7,8,9,10,11,12,20,21,23)

#shows top correlated variables
z <-cor(project_data_final[,quantvars])
z[lower.tri(z,diag=TRUE)]=NA 
z2=as.data.frame(as.table(z))  
z3=na.omit(z2) 
z4 <- subset(z3, abs(Freq)> .5) 
correlation_df <- z4[order(-z4$Freq), ]
correlation_df

library(PerformanceAnalytics)
chart.Correlation(project_data_final[,quantvars],histogram = TRUE) #correlations for quantitative variables

# double "for" loop computes chi-square tests for each set of variables
#using Bonferroni correction for p-value of (alpha)/(number of tests)= .05/325 = .00015, no p-values were significant, signalling no problems with multicollinearity
df.test <- data.frame(Variable1=character(),Variable2=character(),p_value=double(),stringsAsFactors=FALSE)

for(i in 1:ncol(project_data_final)){
  for(j in i:ncol(project_data_final)){
    chi_test <- chisq.test(unlist(project_data_final[,i]),unlist(project_data_final[,j]),simulate.p.value=TRUE)
    p <- chi_test$p.value
    chi_row <- data.frame(Variable1 = colnames(project_data_final[i]), Variable2 = colnames(project_data_final[j]), p_value = p)
    df.test <- rbind(df.test, chi_row)
 }
}
df.test
df_sorted <- df.test[with(df.test, order(p_value)), ]


#Relationships between predictors and response variables
library(gridExtra)
##Function for creating bar graphs (of all quantitative variables) by target
graphs <- function(x){
xvar <- unlist(project_data_final[,x])
plot <- project_data_final %>% ggplot(aes(xvar,fill=Target))+ 
  geom_bar() + 
  facet_grid(. ~ Target) +
  xlab(colnames(project_data_final[x]))
return(plot)
}
plot_list <-lapply(quantvars,FUN=graphs)
plot_list


##Function for creating FREQUENCY graphs (of all quantitative variables) by target
freqgraphs <- function(x){
xvar <- unlist(project_data_final[,x])
plot <- project_data_final %>% ggplot(aes(xvar, colour=Target)) + 
  geom_freqpoly(aes(group = Target)) +
  xlab(colnames(project_data_final[x]))
return(plot)
}
freqplot_list <-lapply(quantvars,FUN=freqgraphs)
freqplot_list

```


## The Statistical Model
Our ideal model would be a model that optimizes overall accuracy, in which it would accurately predict the level of income for an individual and family. The accuracy of the model is important, as it will help social programs and the government decide who needs financial support, and how much of it. We used many different techniques of modeling the data. Trying numerous methods and models helped us choose the one that optimized overall accuracy. The logistic regression models used included multinomial logistic regression using all the remaining variables, the lowest VIF based model, lasso, ridge, backward selection, forward selection, best BIC, best adjusted R^2, and two different ordinal regression models. The lasso model outputted the greatest accuracy of all the models. On top of evaluating the overall accuracies for the models, we summarized the sensitivities and specificities for each model in order to get a better understanding of the predictive power for each model, and exactly what they are telling us. Accuracies for each model were calculated by summing the number of correctly predicted classes divided by the total number of observations. However, this method fails to account for how “close” predictions are to their correct classification. For example, if a true observation falls into poverty level 4 (non-vulnerable households), a misclassification into level 1 is penalized the same as a misclassification into level 3, even though the second classification is closer to the true value. In order to find a value of accuracy that is more representative of the classifications, for each model, we also summarized the micro-average values. These values yield a prediction precision value between 0 and 1, with 1 being the best. A macro-average will compute the accuracy of a model independently for each class and then take the average (treating all classes equally), whereas a micro-average will weigh the contributions of all the classes to compute the average. In a multi-class classification, the micro-average is preferred if there are more observations of one class than the other classes. In this situation, 67% of our data points fall into the level "4" income. This could negatively alter our accuracy value, so in order to put weights on each class, this analysis used the micro-average value rather than the macro-average value. The lasso model outputted the highest micro-average value. Our data comprised of many different types of variables that tried to predict poverty levels. We attacked the study by using multiple models, as stated previously, and the following will dive deeper into each model.

By running a full multinomial model, we got a grasp and beginners understanding on how each variable impacted the predictive power of poverty levels. To begin, we took the approach of running forward and backwards selection, in which each eliminates insignificant variables from the model from their respective direction in the process. To deal with multicollinearity, we tried to execute a model based on the VIF values of the predictor variables. In the statistical world, the general rule of thumb is that multicollinearity exists within variables that have a VIF of ten or greater. So, our group excluded the variables that did possess VIF of ten or higher, and ran the model in hopes of a better accuracy. We only excluded those variables for this model. Also to help address any multicollinearity and remove some variables, we built a ridge regression model. This model suggested, however, to leave in all of the remaining 24 variables used in the initial full model. On top of these models, we ran two different ordinal models that are similar to a normal general linear model estimation. We decided on ordinal regression being a plausible model to attempt because the response variable was in an ordered fashion, with 1 being very poor and 4 being wealthy, and everything in between. Additionally, a lasso regression model was built, which performed both variable selection and regularization in order to produce a model similar to a linear regression model. Two more models attempted were the best BIC model and highest adjusted R-squared model which were obtained from the regression subset selection technique. For the best BIC model, we simply chose the model that outputted the lowest BIC. This is similar to the adjusted R-squared model, in which instead of having the lowest BIC, we chose the one with the highest adjusted R-squared value, where we attempt to be able to explain as much as we can when relating the poverty levels to each independent variable. All in all, most of the accuracies of these models were between 62% and 67%. Some models, such as ridge regression and forward selection, did not find any variables that were insignificant in the model, and therefore, the accuracies and micro-average values are the same for these models. Below is a table showing the overall accuracies and micro-average values for each model attempted in the model building and selection process.

##Additional Analyses
In addition to the models described above, one more method was used to try to find a way to accurately predict poverty class. A Support Vector Machine (SVM) was used to create a model. SVMs are popular in the data science community for classification tasks as they tend to perform well across many domains. SVMs perform classification tasks by constructing hyperplanes in a multidimensional space that separates cases of different class labels. Although the mathematics behind this method are quite complex, the final model built was significantly more accurate than any of the other models in this analysis. In order to maximize the accuracy for the SVM, different values of gamma and cost were tested until the optimal accuracy was found, in this case an accuracy of 99.72%. For this technique, gamma is the argument used by the kernel function to define how far the influence of a single observation reaches and cost allows for control of the cost of misclassification of data. When cost is small, the margins will be wide, resulting in many support vectors. For this analysis, the maximized accuracy was achieved through a gamma value of .7 and a cost of 10. Further analysis would be needed in order to fully understand how the SVM model works and was built in this analysis, in addition to investigating the validity of the accuracy value.


```{r message=FALSE, warning=FALSE, echo=FALSE, include=FALSE}

#split data into raining and testing set
set.seed(46217921)
train.pov <- sample(1:nrow(project_data_final), floor(.75*nrow(project_data_final)), replace=F)
dat.tr <- project_data_final[train.pov,]
dat.te <- project_data_final[-train.pov,]
head(dat.tr)
table(dat.tr$Target) #.66556 proportions are target 4
table(dat.te$Target) #.626995 proportions are target 4


library(nnet)
library(MLmetrics)
library(compositions)
library(caret)

#Build regression model using training/testing
mod.full <- multinom(Target ~ ., data=dat.tr) #Multinom Function

#gives VIF values of each variable in the full model: want to investigate variables with VIF higher than 5-10, for this analyses the following remain after removing higher than 10: v18q1,CementorBetter, cielorazo, epared1, etecho1, eviv1, dis, computer, television, area1
car::vif(mod.full)

#Finds pseudo R^2 value of model
library(DescTools)
PseudoR2(mod.full,"McFadden") # R^2 = .227

#ACCURACY OF TRAINING DATA - Full Multinomial Model = .702
predicted_classFM_TR <- predict(mod.full,dat.tr)
caret::confusionMatrix(as.factor(predicted_classFM_TR),as.factor(dat.tr$Target))

#ACCURACY OF TESTING DATA - Full Multinomial Model= .634
predicted_classFM_TE <- predict(mod.full,dat.te)
caret::confusionMatrix(as.factor(predicted_classFM_TE),as.factor(dat.te$Target))

#SENSITIVITIES, SPECIFICITIES, AND AUC TRAINING DATA - Full Multinomial Model
dat_roc_FM_TR<- data.frame(T1_true = numeric(2114),T2_true= numeric(2114),T3_true= numeric(2114),T4_true= numeric(2114),T1_pred_FM= numeric(2114),T2_pred_FM= numeric(2114),T3_pred_FM= numeric(2114),T4_pred_FM= numeric(2114))
#creates data frame in form needed for multi_roc function
dat_roc_FM_TR$T1_true <- ifelse(dat.tr$Target == 1, 1, 0)
dat_roc_FM_TR$T2_true <- ifelse(dat.tr$Target == 2, 1, 0)
dat_roc_FM_TR$T3_true <- ifelse(dat.tr$Target == 3, 1, 0)
dat_roc_FM_TR$T4_true <- ifelse(dat.tr$Target == 4, 1, 0)
dat_roc_FM_TR$T1_pred_FM <- ifelse(predicted_classFM_TR == 1, 1, 0)
dat_roc_FM_TR$T2_pred_FM <- ifelse(predicted_classFM_TR == 2, 1, 0)
dat_roc_FM_TR$T3_pred_FM <- ifelse(predicted_classFM_TR == 3, 1, 0)
dat_roc_FM_TR$T4_pred_FM <- ifelse(predicted_classFM_TR == 4, 1, 0)

#micro value training data = .7713
rocfit.full_FM_TR <- multi_roc(dat_roc_FM_TR)
rocfit.full_FM_TR$AUC


#SENSITIVITIES, SPECIFICITIES, AND AUC TESTING DATA - Full Multinomial Model
dat_roc_FM_TE<- data.frame(T1_true = numeric(705),T2_true= numeric(705),T3_true= numeric(705),T4_true= numeric(705),T1_pred_FM= numeric(705),T2_pred_FM= numeric(705),T3_pred_FM= numeric(705),T4_pred_FM= numeric(705))
#creates data frame in form needed for multi_roc function
dat_roc_FM_TE$T1_true <- ifelse(dat.te$Target == 1, 1, 0)
dat_roc_FM_TE$T2_true <- ifelse(dat.te$Target == 2, 1, 0)
dat_roc_FM_TE$T3_true <- ifelse(dat.te$Target == 3, 1, 0)
dat_roc_FM_TE$T4_true <- ifelse(dat.te$Target == 4, 1, 0)
dat_roc_FM_TE$T1_pred_FM <- ifelse(predicted_classFM_TE == 1, 1, 0)
dat_roc_FM_TE$T2_pred_FM <- ifelse(predicted_classFM_TE == 2, 1, 0)
dat_roc_FM_TE$T3_pred_FM <- ifelse(predicted_classFM_TE == 3, 1, 0)
dat_roc_FM_TE$T4_pred_FM <- ifelse(predicted_classFM_TE == 4, 1, 0)

#micro value testing data= .7280
rocfit.full_FM_TE <- multi_roc(dat_roc_FM_TE)
rocfit.full_FM_TE$AUC




##Multinomial Model #2
mod.vif_under_10 <-multinom(Target~v18q1 + CementorBetter + cielorazo + epared1 + etecho1 + eviv1 + dis + computer + television + area1, data = dat.tr)

car::vif(mod.vif_under_10)
PseudoR2(mod.vif_under_10,"all") # R^2 = .1026

#ACCURACY OF TRAINING DATA - VIF Under 10 Multinomial Model = .6712
predicted_classFM2_TR <- predict(mod.vif_under_10,dat.tr)
caret::confusionMatrix(as.factor(predicted_classFM2_TR),as.factor(dat.tr$Target))

#ACCURACY OF TESTING DATA - VIF Under 10 Multinomial Model= .6241
predicted_classFM2_TE <- predict(mod.vif_under_10,dat.te)
caret::confusionMatrix(as.factor(predicted_classFM2_TE),as.factor(dat.te$Target))

#SENSITIVITIES, SPECIFICITIES, AND AUC TRAINING DATA- Multinomial Model #2
dat_roc_FM2_TR<- data.frame(T1_true = numeric(2114),T2_true= numeric(2114),T3_true= numeric(2114),T4_true= numeric(2114),T1_pred_FM2= numeric(2114),T2_pred_FM2= numeric(2114),T3_pred_FM2= numeric(2114),T4_pred_FM2= numeric(2114))
#creates data frame in form needed for multi_roc function
dat_roc_FM2_TR$T1_true <- ifelse(dat.tr$Target == 1, 1, 0)
dat_roc_FM2_TR$T2_true <- ifelse(dat.tr$Target == 2, 1, 0)
dat_roc_FM2_TR$T3_true <- ifelse(dat.tr$Target == 3, 1, 0)
dat_roc_FM2_TR$T4_true <- ifelse(dat.tr$Target == 4, 1, 0)
dat_roc_FM2_TR$T1_pred_FM2 <- ifelse(predicted_classFM2_TR == 1, 1, 0)
dat_roc_FM2_TR$T2_pred_FM2 <- ifelse(predicted_classFM2_TR == 2, 1, 0)
dat_roc_FM2_TR$T3_pred_FM2 <- ifelse(predicted_classFM2_TR == 3, 1, 0)
dat_roc_FM2_TR$T4_pred_FM2 <- ifelse(predicted_classFM2_TR == 4, 1, 0)

#micro value training data = .7562
rocfit.full_FM2_TR <- multi_roc(dat_roc_FM2_TR)
rocfit.full_FM2_TR$AUC

#SENSITIVITIES, SPECIFICITIES, AND AUC TESTING DATA - Multinomial Model #2
dat_roc_FM2_TE<- data.frame(T1_true = numeric(705),T2_true= numeric(705),T3_true= numeric(705),T4_true= numeric(705),T1_pred_FM2= numeric(705),T2_pred_FM2= numeric(705),T3_pred_FM2= numeric(705),T4_pred_FM2= numeric(705))
#creates data frame in form needed for multi_roc function
dat_roc_FM2_TE$T1_true <- ifelse(dat.te$Target == 1, 1, 0)
dat_roc_FM2_TE$T2_true <- ifelse(dat.te$Target == 2, 1, 0)
dat_roc_FM2_TE$T3_true <- ifelse(dat.te$Target == 3, 1, 0)
dat_roc_FM2_TE$T4_true <- ifelse(dat.te$Target == 4, 1, 0)
dat_roc_FM2_TE$T1_pred_FM2 <- ifelse(predicted_classFM2_TE == 1, 1, 0)
dat_roc_FM2_TE$T2_pred_FM2 <- ifelse(predicted_classFM2_TE == 2, 1, 0)
dat_roc_FM2_TE$T3_pred_FM2 <- ifelse(predicted_classFM2_TE == 3, 1, 0)
dat_roc_FM2_TE$T4_pred_FM2 <- ifelse(predicted_classFM2_TE == 4, 1, 0)

#micro value testing data = .7252
rocfit.full_FM2_TE <- multi_roc(dat_roc_FM2_TE)
rocfit.full_FM2_TE$AUC
```

```{r}
#Cumulative Logit Model - Reflects ordering 
library(VGAM)
dat.tr$Target <- ordered(dat.tr$Target, levels = c(1,2,3,4))
dat.te$Target <- ordered(dat.te$Target, levels = c(1,2,3,4))
fit.adj_tr <- vglm(Target ~ .,family=cumulative(parallel=TRUE), data=dat.tr)
fit.adj_te <- vglm(Target ~ .,family=cumulative(parallel=TRUE), data=dat.te)
predict.tr <- predict(fit.adj_tr,dat.tr, type = "response")
predict.te <- predict(fit.adj_te,dat.te, type = "response")

#ACCURACY OF TRAINING DATA = .6935
predicted_classVGLM_TR <- apply(predict.tr, 1, which.max)
caret::confusionMatrix(as.factor(predicted_classVGLM_TR),as.factor(dat.tr$Target))

#ACCURACY OF TESTING DATA = .6596
predicted_classVGLM_TE <- apply(predict.te, 1, which.max)
caret::confusionMatrix(as.factor(predicted_classVGLM_TE),as.factor(dat.te$Target))

#SENSITIVITIES, SPECIFICITIES, AND AUC TRAINING DATA - VGLM 
dat_roc_VGLM_TR<- data.frame(T1_true = numeric(2114),T2_true= numeric(2114),T3_true= numeric(2114),T4_true= numeric(2114),T1_pred_VGLM= numeric(2114),T2_pred_VGLM= numeric(2114),T3_pred_VGLM= numeric(2114),T4_pred_VGLM= numeric(2114))
#creates data frame in form needed for multi_roc function
dat_roc_VGLM_TR$T1_true <- ifelse(dat.tr$Target == 1, 1, 0)
dat_roc_VGLM_TR$T2_true <- ifelse(dat.tr$Target == 2, 1, 0)
dat_roc_VGLM_TR$T3_true <- ifelse(dat.tr$Target == 3, 1, 0)
dat_roc_VGLM_TR$T4_true <- ifelse(dat.tr$Target == 4, 1, 0)
dat_roc_VGLM_TR$T1_pred_VGLM <- ifelse(predicted_classVGLM_TR == 1, 1, 0)
dat_roc_VGLM_TR$T2_pred_VGLM <- ifelse(predicted_classVGLM_TR == 2, 1, 0)
dat_roc_VGLM_TR$T3_pred_VGLM <- ifelse(predicted_classVGLM_TR == 3, 1, 0)
dat_roc_VGLM_TR$T4_pred_VGLM <- ifelse(predicted_classVGLM_TR == 4, 1, 0)

#micro value training data = .7647
rocfit.full_VGLM_TR <- multi_roc(dat_roc_VGLM_TR)
rocfit.full_VGLM_TR$AUC

#SENSITIVITIES, SPECIFICITIES, AND AUC TESTING DATA - VGLM 
dat_roc_VGLM_TE<- data.frame(T1_true = numeric(705),T2_true= numeric(705),T3_true= numeric(705),T4_true= numeric(705),T1_pred_VGLM= numeric(705),T2_pred_VGLM= numeric(705),T3_pred_VGLM= numeric(705),T4_pred_VGLM= numeric(705))
#creates data frame in form needed for multi_roc function
dat_roc_VGLM_TE$T1_true <- ifelse(dat.te$Target == 1, 1, 0)
dat_roc_VGLM_TE$T2_true <- ifelse(dat.te$Target == 2, 1, 0)
dat_roc_VGLM_TE$T3_true <- ifelse(dat.te$Target == 3, 1, 0)
dat_roc_VGLM_TE$T4_true <- ifelse(dat.te$Target == 4, 1, 0)
dat_roc_VGLM_TE$T1_pred_VGLM <- ifelse(predicted_classVGLM_TE == 1, 1, 0)
dat_roc_VGLM_TE$T2_pred_VGLM <- ifelse(predicted_classVGLM_TE == 2, 1, 0)
dat_roc_VGLM_TE$T3_pred_VGLM <- ifelse(predicted_classVGLM_TE == 3, 1, 0)
dat_roc_VGLM_TE$T4_pred_VGLM <- ifelse(predicted_classVGLM_TE == 4, 1, 0)

#micro value testing data = .7478
rocfit.full_VGLM_TE<- multi_roc(dat_roc_VGLM_TE)
rocfit.full_VGLM_TE$AUC

```

```{r}
#LASSO MODEL  - suggests to take out hogar_nin, edjefa, and rooms
library(glmnet)
xx <- model.matrix(Target~., dat.tr)
yy <- as.integer(dat.tr$Target) #packages requires this to be numeric
lasso.out <- cv.glmnet(xx, yy, alpha=1, type.measure="class")  
coef(lasso.out, s=lasso.out$lambda.min)
lasso.out

#ACCURACY OF TRAINING DATA = .7006
mod.lasso.tr <- multinom(formula = Target ~ v18q1 + tamviv_sqrt + CementorBetter + cielorazo + GasOrElectric + elimbasu1 + epared1 + etecho1 + eviv1 + dis + comMSS + hogar_adul + hogar_mayor + edjefe + meaneduc + overcrowding + computer + television + qmobilephone + area1 + depend, data = dat.tr)
predicted_classLM_TR <- predict(mod.lasso.tr, type="class")
acc_LM_TR <- caret::confusionMatrix(as.factor(predicted_classLM_TR),as.factor(dat.tr$Target))

#ACCURACY OF Testing DATA = .678
mod.lasso.te <- multinom(formula = Target ~ v18q1 + tamviv_sqrt + CementorBetter + cielorazo + GasOrElectric + elimbasu1 + epared1 + etecho1 + eviv1 + dis + comMSS + hogar_adul + hogar_mayor + edjefe + meaneduc + overcrowding + computer + television + qmobilephone + area1 + depend, data = dat.te)
predicted_classLM_TE <- predict(mod.lasso.te, type="class")
acc_LM_TE <- caret::confusionMatrix(as.factor(predicted_classLM_TE),as.factor(dat.te$Target))

#SENSITIVITIES, SPECIFICITIES, AND AUC TRAINING DATA - LASSO MODEL
dat_roc_tr_LM <- data.frame(T1_true = numeric(2114),T2_true= numeric(2114),T3_true= numeric(2114),T4_true= numeric(2114),T1_pred_LM= numeric(2114),T2_pred_LM= numeric(2114),T3_pred_LM= numeric(2114),T4_pred_LM= numeric(2114))
#creates data frame in form needed for multi_roc function
dat_roc_tr_LM$T1_true <- ifelse(dat.tr$Target == 1, 1, 0)
dat_roc_tr_LM$T2_true <- ifelse(dat.tr$Target == 2, 1, 0)
dat_roc_tr_LM$T3_true <- ifelse(dat.tr$Target == 3, 1, 0)
dat_roc_tr_LM$T4_true <- ifelse(dat.tr$Target == 4, 1, 0)
dat_roc_tr_LM$T1_pred_LM <- ifelse(predicted_classLM_TR == 1, 1, 0)
dat_roc_tr_LM$T2_pred_LM <- ifelse(predicted_classLM_TR == 2, 1, 0)
dat_roc_tr_LM$T3_pred_LM <- ifelse(predicted_classLM_TR == 3, 1, 0)
dat_roc_tr_LM$T4_pred_LM <- ifelse(predicted_classLM_TR == 4, 1, 0)

#micro value training data = .7664
rocfit.full_tr_LM<- multi_roc(dat_roc_tr_LM)
rocfit.full_tr_LM$AUC

#SENSITIVITIES, SPECIFICITIES, AND AUC TESTING DATA - LASSO MODEL
dat_roc_te_LM <- data.frame(T1_true = numeric(705),T2_true= numeric(705),T3_true= numeric(705),T4_true= numeric(705),T1_pred_LM= numeric(705),T2_pred_LM= numeric(705),T3_pred_LM= numeric(705),T4_pred_LM= numeric(705))
#creates data frame in form needed for multi_roc function
dat_roc_te_LM$T1_true <- ifelse(dat.te$Target == 1, 1, 0)
dat_roc_te_LM$T2_true <- ifelse(dat.te$Target == 2, 1, 0)
dat_roc_te_LM$T3_true <- ifelse(dat.te$Target == 3, 1, 0)
dat_roc_te_LM$T4_true <- ifelse(dat.te$Target == 4, 1, 0)
dat_roc_te_LM$T1_pred_LM <- ifelse(predicted_classLM_TE == 1, 1, 0)
dat_roc_te_LM$T2_pred_LM <- ifelse(predicted_classLM_TE == 2, 1, 0)
dat_roc_te_LM$T3_pred_LM <- ifelse(predicted_classLM_TE == 3, 1, 0)
dat_roc_te_LM$T4_pred_LM <- ifelse(predicted_classLM_TE == 4, 1, 0)

#micro value testing data = .7586
library(multiROC)
rocfit.full_te_LM<- multi_roc(dat_roc_te_LM)
rocfit.full_te_LM$AUC
```

```{r}
#Ridge MODEL  - doesn't suggest to take any variables out
xxR <- model.matrix(Target~., dat.tr)
yyR <- as.integer(dat.tr$Target) #packages requires this to be numeric
ridge.out <- cv.glmnet(xxR, yyR, alpha=0, type.measure="class") 
coef(ridge.out, s=ridge.out$lambda.min)

#same accuracies as full multinomial model
#Training Accuracy = .702
#Testing Accuracy = .634
#same micro values of full multinomial model = training=  .7713    testing = .7280

```

```{r}
#Backward Selection - significant variables in equation below
#multinom(formula = Target ~ v18q1 + tamviv + CementorBetter + cielorazo + GasOrElectric + etecho1 + eviv1 + dis + hogar_mayor + edjefe + meaneduc + television + qmobilephone + area1 + depend, data = dat.tr)
mod.back <- step(mod.full,  direction="backward", trace=0)
summary(mod.back)

#ACCURACY OF TRAINING DATA = .7006
predicted_classBS_TR <- predict(mod.back,dat.tr)
caret::confusionMatrix(as.factor(predicted_classBS_TR),as.factor(dat.tr$Target))

#ACCURACY OF TESTING DATA = .6426
predicted_classBS_TE <- predict(mod.back,dat.te)
caret::confusionMatrix(as.factor(predicted_classBS_TE),as.factor(dat.te$Target))

#SENSITIVITIES, SPECIFICITIES, AND AUC TRAINING DATA - Backwards Selection Model
dat_roc_BS_TR<- data.frame(T1_true = numeric(2114),T2_true= numeric(2114),T3_true= numeric(2114),T4_true= numeric(2114),T1_pred_BS= numeric(2114),T2_pred_BS= numeric(2114),T3_pred_BS= numeric(2114),T4_pred_BS= numeric(2114))
#creates data frame in form needed for multi_roc function
dat_roc_BS_TR$T1_true <- ifelse(dat.tr$Target == 1, 1, 0)
dat_roc_BS_TR$T2_true <- ifelse(dat.tr$Target == 2, 1, 0)
dat_roc_BS_TR$T3_true <- ifelse(dat.tr$Target == 3, 1, 0)
dat_roc_BS_TR$T4_true <- ifelse(dat.tr$Target == 4, 1, 0)
dat_roc_BS_TR$T1_pred_BS <- ifelse(predicted_classBS_TR == 1, 1, 0)
dat_roc_BS_TR$T2_pred_BS <- ifelse(predicted_classBS_TR == 2, 1, 0)
dat_roc_BS_TR$T3_pred_BS <- ifelse(predicted_classBS_TR == 3, 1, 0)
dat_roc_BS_TR$T4_pred_BS <- ifelse(predicted_classBS_TR == 4, 1, 0)

#micro value training data = .7700
rocfit.full_BS_TR <- multi_roc(dat_roc_BS_TR)
rocfit.full_BS_TR$AUC

#SENSITIVITIES, SPECIFICITIES, AND AUC TESTING DATA - Backwards Selection Model
dat_roc_BS_TE<- data.frame(T1_true = numeric(705),T2_true= numeric(705),T3_true= numeric(705),T4_true= numeric(705),T1_pred_BS= numeric(705),T2_pred_BS= numeric(705),T3_pred_BS= numeric(705),T4_pred_BS= numeric(705))
#creates data frame in form needed for multi_roc function
dat_roc_BS_TE$T1_true <- ifelse(dat.te$Target == 1, 1, 0)
dat_roc_BS_TE$T2_true <- ifelse(dat.te$Target == 2, 1, 0)
dat_roc_BS_TE$T3_true <- ifelse(dat.te$Target == 3, 1, 0)
dat_roc_BS_TE$T4_true <- ifelse(dat.te$Target == 4, 1, 0)
dat_roc_BS_TE$T1_pred_BS <- ifelse(predicted_classBS_TE == 1, 1, 0)
dat_roc_BS_TE$T2_pred_BS <- ifelse(predicted_classBS_TE == 2, 1, 0)
dat_roc_BS_TE$T3_pred_BS <- ifelse(predicted_classBS_TE == 3, 1, 0)
dat_roc_BS_TE$T4_pred_BS <- ifelse(predicted_classBS_TE == 4, 1, 0)

#micro value testing data = .7327
rocfit.full_BS_TE <- multi_roc(dat_roc_BS_TE)
rocfit.full_BS_TE$AUC
```

```{r}
#Forward Selection - didn't remove any variables, equation below
#multinom(formula = Target ~ rooms + v18q1 + tamviv + CementorBetter + cielorazo + GasOrElectric + elimbasu1 + epared1 + etecho1 + eviv1 + dis + comMSS + hogar_nin + hogar_adul + hogar_mayor + edjefe + edjefa + meaneduc + overcrowding + computer + television + qmobilephone + area1 + depend, data = dat.tr)
mod.forward <- step(mod.full, direction="forward", trace=0)
test <- predict(mod.forward, type="class")
summary(mod.forward)

#ACCURACY OF TRAINING DATA = .702
predicted_classFS_TR <- predict(mod.forward,dat.tr)
caret::confusionMatrix(as.factor(predicted_classFS_TR),as.factor(dat.tr$Target))

#ACCURACY OF TESTING DATA = .634
predicted_classFS_TE <- predict(mod.forward,dat.te)
caret::confusionMatrix(as.factor(predicted_classFS_TE),as.factor(dat.te$Target))

#same micro values of full multinomial model = training=  .7713    testing = .7280
```

```{r}
#REGSUBSETS METHOD
library(leaps)
subsets <- regsubsets(Target ~ . , data = dat.tr, nbest = 1, nvmax = NULL, force.in = NULL, force.out = NULL, method = "exhaustive") 
summary.out <- summary(subsets)  
which.max(summary.out$adjr2) 
which.min(summary.out$bic) 

##BEST ADJ R^2 MODEL
summary.out$which[17,]

#ACCURACY OF TRAINING DATA = .6987
mod.adjr2.tr <- multinom(formula = Target ~ v18q1 + CementorBetter + tamviv_sqrt + cielorazo + GasOrElectric + elimbasu1 + epared1 + etecho1 + dis + hogar_adul + hogar_mayor + edjefe + meaneduc + television + qmobilephone + area1 + depend, data = dat.tr)
predicted_classAR2_TR <- predict(mod.adjr2.tr, type="class")
caret::confusionMatrix(as.factor(predicted_classAR2_TR),as.factor(dat.tr$Target))

#ACCURACY OF Testing DATA = .6667
mod.adjr2.te <- multinom(formula = Target ~ v18q1 + CementorBetter + tamviv_sqrt + cielorazo + GasOrElectric + elimbasu1 + epared1 + etecho1 + dis + hogar_adul + hogar_mayor + edjefe + meaneduc + television + qmobilephone + area1 + depend, data = dat.te)
predicted_classAR2_TE <- predict(mod.adjr2.te, type="class")
caret::confusionMatrix(as.factor(predicted_classAR2_TE),as.factor(dat.te$Target))

#SENSITIVITIES, SPECIFICITIES, AND AUC TRAINING DATA - BEST ADJ R^2 MODEL
dat_roc_tr_AR2 <- data.frame(T1_true = numeric(2114),T2_true= numeric(2114),T3_true= numeric(2114),T4_true= numeric(2114),T1_pred_AR2= numeric(2114),T2_pred_AR2= numeric(2114),T3_pred_AR2= numeric(2114),T4_pred_AR2= numeric(2114))
#creates data frame in form needed for multi_roc function
dat_roc_tr_AR2$T1_true <- ifelse(dat.tr$Target == 1, 1, 0)
dat_roc_tr_AR2$T2_true <- ifelse(dat.tr$Target == 2, 1, 0)
dat_roc_tr_AR2$T3_true <- ifelse(dat.tr$Target == 3, 1, 0)
dat_roc_tr_AR2$T4_true <- ifelse(dat.tr$Target == 4, 1, 0)
dat_roc_tr_AR2$T1_pred_AR2 <- ifelse(predicted_classAR2_TR == 1, 1, 0)
dat_roc_tr_AR2$T2_pred_AR2 <- ifelse(predicted_classAR2_TR == 2, 1, 0)
dat_roc_tr_AR2$T3_pred_AR2 <- ifelse(predicted_classAR2_TR == 3, 1, 0)
dat_roc_tr_AR2$T4_pred_AR2 <- ifelse(predicted_classAR2_TR == 4, 1, 0)

#micro value training data = .7675
rocfit.full_tr_AR2<- multi_roc(dat_roc_tr_AR2)
rocfit.full_tr_AR2$AUC

#SENSITIVITIES, SPECIFICITIES, AND AUC TESTING DATA - BEST ADJ R^2 MODEL
dat_roc_te_AR2 <- data.frame(T1_true = numeric(705),T2_true= numeric(705),T3_true= numeric(705),T4_true= numeric(705),T1_pred_AR2= numeric(705),T2_pred_AR2= numeric(705),T3_pred_AR2= numeric(705),T4_pred_AR2= numeric(705))
#creates data frame in form needed for multi_roc function
dat_roc_te_AR2$T1_true <- ifelse(dat.te$Target == 1, 1, 0)
dat_roc_te_AR2$T2_true <- ifelse(dat.te$Target == 2, 1, 0)
dat_roc_te_AR2$T3_true <- ifelse(dat.te$Target == 3, 1, 0)
dat_roc_te_AR2$T4_true <- ifelse(dat.te$Target == 4, 1, 0)
dat_roc_te_AR2$T1_pred_AR2 <- ifelse(predicted_classAR2_TE == 1, 1, 0)
dat_roc_te_AR2$T2_pred_AR2 <- ifelse(predicted_classAR2_TE == 2, 1, 0)
dat_roc_te_AR2$T3_pred_AR2 <- ifelse(predicted_classAR2_TE == 3, 1, 0)
dat_roc_te_AR2$T4_pred_AR2 <- ifelse(predicted_classAR2_TE == 4, 1, 0)

#micro value testing data = .7484
rocfit.full_te_AR2<- multi_roc(dat_roc_te_AR2)
rocfit.full_te_AR2$AUC



#BEST BIC MODEL
summary.out$which[12,] 

#ACCURACY OF TRAINING DATA = .6944
mod.bic.tr <- multinom(formula = Target ~ tamviv_sqrt + cielorazo + GasOrElectric + etecho1 + dis + hogar_mayor + edjefe + meaneduc + television + qmobilephone + area1 + depend, data = dat.tr)
predicted_classBIC_TR <- predict(mod.bic.tr, type="class")
caret::confusionMatrix(as.factor(predicted_classBIC_TR),as.factor(dat.tr$Target))

#ACCURACY OF Testing DATA = .6567
mod.bic.te <- multinom(formula = Target ~ tamviv_sqrt + cielorazo + GasOrElectric + etecho1 + dis + hogar_mayor + edjefe + meaneduc + television + qmobilephone + area1 + depend, data = dat.te)
predicted_classBIC_TE <- predict(mod.bic.te, type="class")
caret::confusionMatrix(as.factor(predicted_classBIC_TE),as.factor(dat.te$Target))

#SENSITIVITIES, SPECIFICITIES, AND AUC TRAINING DATA - BEST BIC MODEL
dat_roc_tr_BIC <- data.frame(T1_true = numeric(2114),T2_true= numeric(2114),T3_true= numeric(2114),T4_true= numeric(2114),T1_pred_BIC= numeric(2114),T2_pred_BIC= numeric(2114),T3_pred_BIC= numeric(2114),T4_pred_BIC= numeric(2114))
#creates data frame in form needed for multi_roc function
dat_roc_tr_BIC$T1_true <- ifelse(dat.tr$Target == 1, 1, 0)
dat_roc_tr_BIC$T2_true <- ifelse(dat.tr$Target == 2, 1, 0)
dat_roc_tr_BIC$T3_true <- ifelse(dat.tr$Target == 3, 1, 0)
dat_roc_tr_BIC$T4_true <- ifelse(dat.tr$Target == 4, 1, 0)
dat_roc_tr_BIC$T1_pred_BIC <- ifelse(predicted_classBIC_TR == 1, 1, 0)
dat_roc_tr_BIC$T2_pred_BIC <- ifelse(predicted_classBIC_TR == 2, 1, 0)
dat_roc_tr_BIC$T3_pred_BIC <- ifelse(predicted_classBIC_TR == 3, 1, 0)
dat_roc_tr_BIC$T4_pred_BIC <- ifelse(predicted_classBIC_TR == 4, 1, 0)

#micro value training data = .7649
rocfit.full_tr_BIC<- multi_roc(dat_roc_tr_BIC)
rocfit.full_tr_BIC$AUC

#SENSITIVITIES, SPECIFICITIES, AND AUC TESTING DATA - BEST BIC MODEL
dat_roc_te_BIC <- data.frame(T1_true = numeric(705),T2_true= numeric(705),T3_true= numeric(705),T4_true= numeric(705),T1_pred_BIC= numeric(705),T2_pred_BIC= numeric(705),T3_pred_BIC= numeric(705),T4_pred_BIC= numeric(705))
#creates data frame in form needed for multi_roc function
dat_roc_te_BIC$T1_true <- ifelse(dat.te$Target == 1, 1, 0)
dat_roc_te_BIC$T2_true <- ifelse(dat.te$Target == 2, 1, 0)
dat_roc_te_BIC$T3_true <- ifelse(dat.te$Target == 3, 1, 0)
dat_roc_te_BIC$T4_true <- ifelse(dat.te$Target == 4, 1, 0)
dat_roc_te_BIC$T1_pred_BIC <- ifelse(predicted_classBIC_TE == 1, 1, 0)
dat_roc_te_BIC$T2_pred_BIC <- ifelse(predicted_classBIC_TE == 2, 1, 0)
dat_roc_te_BIC$T3_pred_BIC <- ifelse(predicted_classBIC_TE == 3, 1, 0)
dat_roc_te_BIC$T4_pred_BIC <- ifelse(predicted_classBIC_TE == 4, 1, 0)

#micro value testing data = .7408
rocfit.full_te_BIC<- multi_roc(dat_roc_te_BIC)
rocfit.full_te_BIC$AUC

```

```{r}
#ORDINAL MODELS

library(MASS)
library(ordinal)
library(erer)

##ORDINAL MODEL #1
ord1<-clm(formula=(Target ~.), data= dat.tr)
summary(ord)

#ACCURACY of ordinal model #1 (training) =.6935
predicted_class1<- predict(ord1, dat.tr, type = "class")
predicted_classOR1_TR<- ordered(predicted_class1$fit, levels = c(1,2,3,4))
cm_ols <- table(predicted_classOR1_TR, dat.tr$Target)
print(cm_ols)
Accuracytr <- sum(diag(cm_ols))/sum(cm_ols)
Accuracytr

#ACCURACY of ordinal model #1 (testing) = .6468
predicted_class2<- predict(ord1, dat.te, type = "class")
predicted_classOR1_TE<- ordered(predicted_class2$fit, levels = c(1,2,3,4))
cm_ols2 <- table(predicted_classOR1_TE, dat.te$Target)
print(cm_ols2)
Accuracyte <- sum(diag(cm_ols2))/sum(cm_ols2)
Accuracyte


#SENSITIVITIES, SPECIFICITIES, AND AUC TRAINING DATA - Ordinal Model #1
dat_roc_OR1_TR<- data.frame(T1_true = numeric(2114),T2_true= numeric(2114),T3_true= numeric(2114),T4_true= numeric(2114),T1_pred_OR1= numeric(2114),T2_pred_OR1= numeric(2114),T3_pred_OR1= numeric(2114),T4_pred_OR1= numeric(2114))
#creates data frame in form needed for multi_roc function
dat_roc_OR1_TR$T1_true <- ifelse(dat.tr$Target == 1, 1, 0)
dat_roc_OR1_TR$T2_true <- ifelse(dat.tr$Target == 2, 1, 0)
dat_roc_OR1_TR$T3_true <- ifelse(dat.tr$Target == 3, 1, 0)
dat_roc_OR1_TR$T4_true <- ifelse(dat.tr$Target == 4, 1, 0)
dat_roc_OR1_TR$T1_pred_OR1 <- ifelse(predicted_classOR1_TR == 1, 1, 0)
dat_roc_OR1_TR$T2_pred_OR1 <- ifelse(predicted_classOR1_TR == 2, 1, 0)
dat_roc_OR1_TR$T3_pred_OR1 <- ifelse(predicted_classOR1_TR == 3, 1, 0)
dat_roc_OR1_TR$T4_pred_OR1 <- ifelse(predicted_classOR1_TR == 4, 1, 0)

#micro-average value training data = .7647
rocfit.full_OR1_TR <- multi_roc(dat_roc_OR1_TR)
rocfit.full_OR1_TR$AUC


#SENSITIVITIES, SPECIFICITIES, AND AUC TESTING DATA - Ordinal Model #1
dat_roc_OR1_TE<- data.frame(T1_true = numeric(705),T2_true= numeric(705),T3_true= numeric(705),T4_true= numeric(705),T1_pred_OR1= numeric(705),T2_pred_OR1= numeric(705),T3_pred_OR1= numeric(705),T4_pred_OR1= numeric(705))
#creates data frame in form needed for multi_roc function
dat_roc_OR1_TE$T1_true <- ifelse(dat.te$Target == 1, 1, 0)
dat_roc_OR1_TE$T2_true <- ifelse(dat.te$Target == 2, 1, 0)
dat_roc_OR1_TE$T3_true <- ifelse(dat.te$Target == 3, 1, 0)
dat_roc_OR1_TE$T4_true <- ifelse(dat.te$Target == 4, 1, 0)
dat_roc_OR1_TE$T1_pred_OR1 <- ifelse(predicted_classOR1_TE == 1, 1, 0)
dat_roc_OR1_TE$T2_pred_OR1 <- ifelse(predicted_classOR1_TE == 2, 1, 0)
dat_roc_OR1_TE$T3_pred_OR1 <- ifelse(predicted_classOR1_TE == 3, 1, 0)
dat_roc_OR1_TE$T4_pred_OR1 <- ifelse(predicted_classOR1_TE == 4, 1, 0)


#micro-average value testing data = .7319
rocfit.full_OR1_TE <- multi_roc(dat_roc_OR1_TE)
rocfit.full_OR1_TE$AUC
 

ord2 <- clm(formula = (Target ~ v18q1 + tamviv_sqrt + CementorBetter + cielorazo + GasOrElectric + etecho1 + dis + hogar_mayor + edjefe + meaneduc + television + qmobilephone + area1 + depend), data = dat.tr)
summary(ord2)

#ACCURACY of ordinal model #2 (training) =.6944
predicted_class3<- predict(ord2, dat.tr, type = "class")
predicted_classOR2_TR<- ordered(predicted_class3$fit, levels = c(1,2,3,4))
cm_ols3 <- table(predicted_classOR2_TR, dat.tr$Target)
print(cm_ols3)
Accuracytr2 <- sum(diag(cm_ols3))/sum(cm_ols3)
Accuracytr2

#ACCURACY of ordinal model #2 (testing) = .6426
predicted_class4<- predict(ord2, dat.te, type = "class")
predicted_classOR2_TE<- ordered(predicted_class4$fit, levels = c(1,2,3,4))
cm_ols4 <- table(predicted_classOR2_TE, dat.te$Target)
print(cm_ols4)
Accuracyte <- sum(diag(cm_ols4))/sum(cm_ols4)
Accuracyte

#SENSITIVITIES, SPECIFICITIES, AND AUC TRAINING DATA - Ordinal Model #2
dat_roc_OR2_TR<- data.frame(T1_true = numeric(2114),T2_true= numeric(2114),T3_true= numeric(2114),T4_true= numeric(2114),T1_pred_OR2= numeric(2114),T2_pred_OR2= numeric(2114),T3_pred_OR2= numeric(2114),T4_pred_OR2= numeric(2114))
#creates data frame in form needed for multi_roc function
dat_roc_OR2_TR$T1_true <- ifelse(dat.tr$Target == 1, 1, 0)
dat_roc_OR2_TR$T2_true <- ifelse(dat.tr$Target == 2, 1, 0)
dat_roc_OR2_TR$T3_true <- ifelse(dat.tr$Target == 3, 1, 0)
dat_roc_OR2_TR$T4_true <- ifelse(dat.tr$Target == 4, 1, 0)
dat_roc_OR2_TR$T1_pred_OR2 <- ifelse(predicted_classOR2_TR == 1, 1, 0)
dat_roc_OR2_TR$T2_pred_OR2 <- ifelse(predicted_classOR2_TR == 2, 1, 0)
dat_roc_OR2_TR$T3_pred_OR2 <- ifelse(predicted_classOR2_TR == 3, 1, 0)
dat_roc_OR2_TR$T4_pred_OR2 <- ifelse(predicted_classOR2_TR == 4, 1, 0)

#micro-average value training data = .7652
rocfit.full_OR2_TR <- multi_roc(dat_roc_OR2_TR)
rocfit.full_OR2_TR$AUC

#SENSITIVITIES, SPECIFICITIES, AND AUC TESTING DATA - Ordinal Model #2
dat_roc_OR2_TE<- data.frame(T1_true = numeric(705),T2_true= numeric(705),T3_true= numeric(705),T4_true= numeric(705),T1_pred_OR2= numeric(705),T2_pred_OR2= numeric(705),T3_pred_OR2= numeric(705),T4_pred_OR2= numeric(705))
#creates data frame in form needed for multi_roc function
dat_roc_OR2_TE$T1_true <- ifelse(dat.te$Target == 1, 1, 0)
dat_roc_OR2_TE$T2_true <- ifelse(dat.te$Target == 2, 1, 0)
dat_roc_OR2_TE$T3_true <- ifelse(dat.te$Target == 3, 1, 0)
dat_roc_OR2_TE$T4_true <- ifelse(dat.te$Target == 4, 1, 0)
dat_roc_OR2_TE$T1_pred_OR2 <- ifelse(predicted_classOR2_TE == 1, 1, 0)
dat_roc_OR2_TE$T2_pred_OR2 <- ifelse(predicted_classOR2_TE == 2, 1, 0)
dat_roc_OR2_TE$T3_pred_OR2 <- ifelse(predicted_classOR2_TE == 3, 1, 0)
dat_roc_OR2_TE$T4_pred_OR2 <- ifelse(predicted_classOR2_TE == 4, 1, 0)

#micro-accuracy value testing data = .7295
rocfit.full_OR2_TE <- multi_roc(dat_roc_OR2_TE)
rocfit.full_OR2_TE$AUC
```

```{r}
#SVM (Support Vector Machines) Model 
library(e1071)

# Accuracy of training data = .9972
modelsvm_tr <- svm(Target ~ ., data=dat.tr, gamma = .7, cost = 10)
summary(modelsvm_tr)
predicted_classSVM_TR <- predict(modelsvm_tr, type="class")
caret::confusionMatrix(as.factor(predicted_classSVM_TR),as.factor(dat.tr$Target))

# Accuracy of testing data = .9972
modelsvm_te <- svm(Target ~ ., data=dat.te, gamma = .7, cost = 10)
summary(modelsvm_te)
predicted_classSVM_TE <- predict(modelsvm_te, type="class")
caret::confusionMatrix(as.factor(predicted_classSVM_TE),as.factor(dat.te$Target))

#SENSITIVITIES, SPECIFICITIES, AND AUC TRAINING DATA - SVM
dat_roc_tr_SVM <- data.frame(T1_true = numeric(2114),T2_true= numeric(2114),T3_true= numeric(2114),T4_true= numeric(2114),T1_pred_SVM = numeric(2114),T2_pred_SVM = numeric(2114),T3_pred_SVM = numeric(2114),T4_pred_SVM = numeric(2114))
#creates data frame in form needed for multi_roc function
dat_roc_tr_SVM$T1_true <- ifelse(dat.tr$Target == 1, 1, 0)
dat_roc_tr_SVM$T2_true <- ifelse(dat.tr$Target == 2, 1, 0)
dat_roc_tr_SVM$T3_true <- ifelse(dat.tr$Target == 3, 1, 0)
dat_roc_tr_SVM$T4_true <- ifelse(dat.tr$Target == 4, 1, 0)
dat_roc_tr_SVM$T1_pred_SVM <- ifelse(predicted_classSVM_TR == 1, 1, 0)
dat_roc_tr_SVM$T2_pred_SVM <- ifelse(predicted_classSVM_TR == 2, 1, 0)
dat_roc_tr_SVM$T3_pred_SVM <- ifelse(predicted_classSVM_TR == 3, 1, 0)
dat_roc_tr_SVM$T4_pred_SVM <- ifelse(predicted_classSVM_TR == 4, 1, 0)

#micro value training data = .9974
rocfit.full_tr_SVM<- multi_roc(dat_roc_tr_SVM)
rocfit.full_tr_SVM$AUC

#SENSITIVITIES, SPECIFICITIES, AND AUC TESTING DATA - SVM
dat_roc_te_SVM <- data.frame(T1_true = numeric(705),T2_true= numeric(705),T3_true= numeric(705),T4_true= numeric(705),T1_pred_SVM= numeric(705),T2_pred_SVM= numeric(705),T3_pred_SVM= numeric(705),T4_pred_SVM= numeric(705))
#creates data frame in form needed for multi_roc function
dat_roc_te_SVM$T1_true <- ifelse(dat.te$Target == 1, 1, 0)
dat_roc_te_SVM$T2_true <- ifelse(dat.te$Target == 2, 1, 0)
dat_roc_te_SVM$T3_true <- ifelse(dat.te$Target == 3, 1, 0)
dat_roc_te_SVM$T4_true <- ifelse(dat.te$Target == 4, 1, 0)
dat_roc_te_SVM$T1_pred_SVM <- ifelse(predicted_classSVM_TE == 1, 1, 0)
dat_roc_te_SVM$T2_pred_SVM <- ifelse(predicted_classSVM_TE == 2, 1, 0)
dat_roc_te_SVM$T3_pred_SVM <- ifelse(predicted_classSVM_TE == 3, 1, 0)
dat_roc_te_SVM$T4_pred_SVM <- ifelse(predicted_classSVM_TE == 4, 1, 0)

#micro value testing data = .9980
rocfit.full_te_SVM<- multi_roc(dat_roc_te_SVM)
rocfit.full_te_SVM$AUC
```
```{r}
#TABLE SHOWING ACCURACIES 
tab <- matrix(c(0.997, 0.998, 0.678, 0.759, 0.667, 0.748, 0.657, 0.741, 0.647, .732, 0.643, .730, 0.643, 0.733, 0.634, 0.728, 0.634, 0.728, 0.634, 0.728, 0.624, 0.725), nrow=11, byrow=TRUE)
rownames(tab) <-(Model=c("SVM", "Lasso", "Best Adj R^2", "Best BIC", "Ordinal #1", "Ordinal #2", "Backward Selection", "Full", "Ridge", "Forward Selection", "VIF Based"))
colnames(tab) <- c("Accuracy", "Micro-Average Value")
as.table(tab)

```


  ##Results Summary 
Many different models were built using various model selection techniques in this analysis. Although, the final suggested model to maximize overall accuracy is the lasso regression model with 21 variables: number of tablets the household owns, square root of the number of persons living in the household, predominant material on the outside wall, if the house has a ceiling, main source of energy used for cooking, method of garbage disposal, condition of walls, condition of roof, condition of floors, if the head of household is disabled, marriage status, number of adults (19-64) in the household, number of adults (over 65) in the household, years of education if the head of household is male, average years of education for adults over 18, number of people per room, if the household has a laptop or desktop computer, if the household has a TV, the number of mobile phones the household owns, if the house is in an urban area, and the dependency rate (number of members of the household younger than 19 or older than 64)/(number of member of household between 19 and 64). The overall accuracy of this final model is 0.678. In other words, this model correctly predicts a poverty class 67.8% of the time. This model had a micro-average value of .7586 which suggests that the weighted accuracy is improved and potentially shows this model had a better classification rate than the standard method of calculating accuracy is showing. Overall, this model seems to have okay predictive power and could be used as a tool for the government and social programs to assign poverty levels to household and give aid accordingly.
In addition, the SVM model yielded an excellent accuracy of .9972 and a micro-average value of .998. This model was built using 24 variables – the same 21 variables mentioned above in addition to the number of children (under 19) in the household, the years of education if the head of household is female, and the number of rooms in the house. This model could serve as a very accurate and powerful predictor, but more analyses would need to be done to verify this technique and its application in this data set. 
In the end, it would be highly beneficial to discover a model that has excellent predictive ability to classify a household or individual into a specific poverty level. This information, using observable household attributes rather than income and expense records, could help give the poorest of the poor the aid and support they need yet often times are not able to receive. This analysis presents a complex and challenging task, but with further investigation and exploration it could lead to a model that is more accurate and practical. 
