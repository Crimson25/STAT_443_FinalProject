---
title: "Final Project"
author: "Kiera Grant, Braden Barglind, and Colin Fitzpatrick"
date: "12/4/2020"
output:
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
The objective of this study is to determine an appropriate logistic regression model in order to classify individuals into one of four poverty levels. Many social programs have a hard time making sure the right people are given enough aid. Often times the poorest of the population might not be given enough aid because they typically cannot provide the necessary income and expense records to prove that they qualify. Instead, this study will consider a familyâ€™s observable household attributes such as the material of their walls and ceiling, or the assets found in the home, to classify poverty levels. The original dataset contains survey responses from 9,057 individuals with 142 predictor variables. For purposes of this study, only data from the "heads of household" will be used which focuses the data down to 2,819 observations. 

In order to find a sufficient model, all of the predictor variables will be assessed for repetitiveness and skewness and potentially transformed. Relationships between predictor variables and poverty level as well as between the different predictor variables will need to be evaluated. This will help determine any preliminary multicollinearity issues so that these variables can be removed from the dataset early. The data will be split into a training data set as well as a validation data set in order to test the accuracy of our model. A logistic regression model will be created using the training data. The model will then be used to predict the poverty level of samples to determine the effectiveness of the model. The model selection tools will be used to determine if more accuracy is gained through a new model. Next, the classification threshold will be investigated to determine which value optimizes accuracy. The final model will be determined based off of these analyses in hopes to have an appropriate model to classify individuals into a poverty class. 

## Exploring and Transforming the Data
The first thing that we had to take care of, was limiting the amount of variables to a sizable amount.  We then deleted all redundant variables which measured the same things but called by different names.  An example of this is these redundant variables hhsize and tamhog, which both measure the size of the household.  So we are able to get rid of one of them and keep the other.  Another thing we did with the data is to combine some of the terms so that we would have one variable that was now a binomial response. An example of this is, there was a male and female tab, which we are able to remove one of them because it gives redundant information.  Another way we partioned through the data was to get rid of variables based in the percentage of reponses that they had.  For example, one of the identifiers was if the house had a bathroom or not and the probability of houses having a bathroom was .995.  This results in a bad predictor as too many people have a bathroom so it is not a good predictor in that since.  So after those three ideas took place we were left with 34 variables left out of 142.
```{r message=FALSE, warning=FALSE, echo=FALSE, include=FALSE}
library(tidyverse) 
library(ggrepel) 
library(kableExtra) 
library(RColorBrewer) 
library(corrplot) 
library(caret) 
library(e1071) 
library(dominanceanalysis) 
library("readxl")


project_data = read_excel("HeadOfHouseHoldFile.xlsx")
head(project_data)

project_data2 <- read_excel("HeadOfHouseHoldFileUpdate.xlsx")
head(project_data2)

##CLEAN UP DATA
#removes reference variables
project_data2 <- project_data2 %>% dplyr::select(-c("paredblolad","paredpreb", "energcocinar2", "energcocinar3", "Married/Free Coupled", "div,sep,wid", "single", "parentesco1"))

apply(is.na(project_data2), 2, which) #detects NAs in data frame, 3 values in meaneduc (1287, 1400, 1455), 1 value in depend (1287)
project_data2[is.na(project_data2)]=0  #replaces NAs with 0 

project_data2$edjefe <- as.integer(project_data2$edjefe) #to change these two variables from char to ints
project_data2$edjefa <- as.integer(project_data2$edjefa)

project_data <- project_data %>% mutate(hogar_nin=replace(hogar_nin, hogar_nin==0, .001)) #how we can change variables with zero for transformations

quantvars <- c(1,2,3,13,14,15,16,17,18,19,22,25)  #updated: of project_data2
categorvars <- c(4,5,6,7,8,9,10,11,12,20,21,23)   #updated: of project_data2

##RELATIONSHIPS
##CORRELATIONS (relationships beween predictors)
#shows top correlated variables
z <-cor(project_data2[,quantvars])
z[lower.tri(z,diag=TRUE)]=NA 
z2=as.data.frame(as.table(z))  
z3=na.omit(z2) 
z4 <- subset(z3, abs(Freq)> .5) 
correlation_df <- z4[order(-z4$Freq), ]
correlation_df

library(PerformanceAnalytics)
chart.Correlation(project_data2[,quantvars],histogram = FALSE) #correlations for quantitative variables
chart.Correlation(project_data2[,categorvars],histogram = FALSE, method = "spearman") #correlations for categorical variables

##RELATIONSHIPS BETWEEN PREDICTORS AND RESPONSE
library(gridExtra)
#Function for creating bar graphs (of all quantitative variables) by target
graphs <- function(x){
xvar <- unlist(project_data2[,x])
plot <- project_data2 %>% ggplot(aes(xvar,fill=Target))+ 
  geom_bar() + 
  facet_grid(. ~ Target) +
  xlab(colnames(project_data2[x]))
return(plot)
}
plot_list <-lapply(quantvars,FUN=graphs)
plot list

#Function for creating FREQUENCY graphs (of all quantitative variables) by target
freqgraphs <- function(x){
xvar <- unlist(project_data2[,x])
plot <- project_data2 %>% ggplot(aes(xvar, colour=Target)) + 
  geom_freqpoly(aes(group = Target)) +
  xlab(colnames(project_data2[x]))
return(plot)
}
freqplot_list <-lapply(quantvars,FUN=freqgraphs)
freqplot_list


#PLOTTING QUANT VARIABLES -> TRANSFORMATIONS

#Rooms Plot (normal)
roomcounts <- table(project_data$rooms)
barplot(roomcounts,main="Number of Rooms", xlab="Room Counts")

#Number of Tablets Plot (heavy skew right)
tabletscounts <- table(project_data$v18q1)
barplot(tabletscounts,main="Number of Tablets", xlab="Tablet Counts")

#Number of People in Household Plot (skew right)
peoplecounts <- table(project_data$tamviv)
barplot(peoplecounts, main="Number of People in Household", xlab="People Counts")
          
#Household Size Plot (small skew right)
hhsizecounts <- table(project_data$hhsize)
barplot(hhsizecounts, main="Household Size", xlab="Household Size Counts")

#Children 0-19 Plot (skew right)
childrencounts <- table(project_data$hogar_nin)
barplot(childrencounts, main="Number of Children 0-19", xlab="Children Counts")

#Number of Adults Plot (small skew right)
adultcounts <- table(project_data$hogar_adul)
barplot(adultcounts, main="Number of Adults", xlab="Adult Counts")

#People 65+ Plot (heavy skew right)
people65counts <- table(project_data$hogar_mayor)
barplot(people65counts, main="People 65+", xlab = "People Counts")

#Total People in Household Plot (small skew right)
totalpeoplecounts <- table(project_data$hogar_total)
barplot(totalpeoplecounts, main="Total People in Household", xlab="People Counts")

#EdJefe Plot (weird)
edjefecounts <- table(project_data$edjefe)
barplot(edjefecounts, main="Ed Jefe Counts", xlab = "Counts")

#EdJefa Plot (weird)
edjefacounts <- table(project_data$edjefa)
barplot(edjefacounts, main="Ed Jefa Counts", xlab="Counts")

#Mean Education Plot (weird)
meaneducounts <- table(project_data$meaneduc)
barplot(meaneducounts, main="Mean Education", xlab="Counts")

#Overcrowding Plot (weird)
overcrowdcounts <- table(project_data$overcrowding)
barplot(overcrowdcounts, main="Overcrowding", xlab="Counts")

#Number of Dependents (weird)
dependcounts <- table(project_data$depend)
barplot(dependcounts, main="Dependencies", xlab="Counts")

#TRANSFORMATIONS: All variables without code are either factors or couldn't be done using log, sqrt, or cube root. Rooms and Number of Adults are normal.

#Number of people in Household transformation: Square root 
sqrttamviv <- (sqrt(project_data$tamviv))
hist(sqrttamviv)

#Household Size transformation: Square root 
sqrthhsize <- sqrt(project_data$hhsize)
hist(sqrthhsize)

#Total people in Household transformation: Square root (can delete this one as we already have tamviv. Tamviv p-value = 0.02 and hogar_total = NA)
sqrthogar_total <- sqrt(project_data$hogar_total)
hist(sqrthogar_total)

#Split data into 60/40 training/testing
set.seed(12345)
train.index <- sample(row.names(project_data), 0.75*dim(project_data)[1])
valid.index <- setdiff(row.names(project_data), train.index) 
train.df <- project_data[train.index,]
valid.df <- project_data[valid.index,]

```


## The Statistical Model
```{r message=FALSE, warning=FALSE, echo=FALSE, include=FALSE}

#split data into raining and testing set
set.seed(46217921)
train.pov <- sample(1:nrow(project_data2), floor(.75*nrow(project_data2)), replace=F)
dat.tr <- project_data2[train.pov,]
dat.te <- project_data2[-train.pov,]
head(dat.tr)

#INITIAL MODEL W/O REMOVING MORE VARIABLES
library(nnet)
mod.full <- multinom(Target ~ ., data=dat.tr) #Multinom Function
summary(mod.full)

library(caret)
#ACCURACY OF TRAINING DATA = .702
predictedFM <- predict(mod.full,dat.tr,na.action =na.pass, type="class")
predicted_classFM <- predict(mod.full,dat.tr)
caret::confusionMatrix(as.factor(predicted_classML),as.factor(dat.tr$Target))

#ACCURACY OF TESTING DATA = .6397
predictedML <- predict(mod.full,dat.te,na.action =na.pass, type="class")
predicted_classML <- predict(mod.full,dat.te)
caret::confusionMatrix(as.factor(predicted_classML),as.factor(dat.te$Target))

#SENSITIVITIES, SPECIFICITIES, AND AUC of initial full model
library(multiROC)
dat_roc <- data.frame(T1_true = numeric(2114),T2_true= numeric(2114),T3_true= numeric(2114),T4_true= numeric(2114),T1_pred_FM= numeric(2114),T2_pred_FM= numeric(2114),T3_pred_FM= numeric(2114),T4_pred_FM= numeric(2114))
#creates data frame in form needed for multi_roc function
dat_roc$T1_true <- ifelse(dat.tr$Target == 1, 1, 0)
dat_roc$T2_true <- ifelse(dat.tr$Target == 2, 1, 0)
dat_roc$T3_true <- ifelse(dat.tr$Target == 3, 1, 0)
dat_roc$T4_true <- ifelse(dat.tr$Target == 4, 1, 0)
dat_roc$T1_pred_FM <- ifelse(preds.tr == 1, 1, 0)
dat_roc$T2_pred_FM <- ifelse(preds.tr == 2, 1, 0)
dat_roc$T3_pred_FM <- ifelse(preds.tr == 3, 1, 0)
dat_roc$T4_pred_FM <- ifelse(preds.tr == 4, 1, 0)

rocfit.full <- multi_roc(dat_roc)

#ORDINAL MODEL 
library(MASS)
library(ordinal)
library(erer)
dat.tr$Target <- ordered(dat.tr$Target, levels = c(1,2,3,4))
dat.te$Target <- ordered(dat.te$Target, levels = c(1,2,3,4))
ord<-clm(formula= (Target~ .), data= dat.tr)
summary(ord)

#ACCURACY of ordinal model #1 (training) =.6944
predicted_class<- predict (ord, dat.tr, type = "class")
predicted_class<- ordered(predicted_class$fit, levels = c(1,2,3,4))
cm_ols <- table(predicted_class, dat.tr$Target)
print(cm_ols)
Accuracytr1 <- sum(diag(cm_ols))/sum(cm_ols)
Accuracytr1

#ACCURACY of ordinal model #1 (testing) = .6496
predicted_class<- predict (ord, dat.te, type = "class")
predicted_class<- ordered(predicted_class$fit, levels = c(1,2,3,4))
cm_ols <- table(predicted_class, dat.te$Target)
print(cm_ols)
Accuracyte1 <- sum(diag(cm_ols))/sum(cm_ols)
Accuracyte1

#removed insignificant variables from ord to build ord2
ord2 <- clm(formula = (Target~ v18q1 + tamviv + CementorBetter + cielorazo + GasOrElectric + etecho1 + dis + hogar_mayor + edjefe + meaneduc + television + qmobilephone + area1 + depend), data = dat.tr)
summary(ord2)

#ACCURACY of ordinal model #2 (training) =.6939
predicted_class<- predict (ord2, dat.tr, type = "class")
predicted_class<- ordered(predicted_class$fit, levels = c(1,2,3,4))
cm_ols <- table(predicted_class, dat.tr$Target)
print(cm_ols)
Accuracytr2 <- sum(diag(cm_ols))/sum(cm_ols)
Accuracytr2

#ACCURACY of ordinal model #2 (testing) = .6440
predicted_class<- predict (ord2, dat.te, type = "class")
predicted_class<- ordered(predicted_class$fit, levels = c(1,2,3,4))
cm_ols <- table(predicted_class, dat.te$Target)
print(cm_ols)
Accuracyte2 <- sum(diag(cm_ols))/sum(cm_ols)
Accuracyte2

#Cumulative Logit Model (reflects ordering)
library(VGAM)
fit.adj <- vglm(Target ~ .,family=acat(reverse=TRUE, parallel=TRUE), data=dat.tr)
coef(fit.adj)
predict.te <- predict(fit.adj,dat.te, type = "response")

#LASSO MODEL  - suggests to take out hogar_nin and edjefa
library(glmnet)
xx <- model.matrix(Target~., dat.tr)
yy <- as.integer(dat.tr$Target) #packages requires this to be numeric
lasso.out <- cv.glmnet(xx, yy, alpha=1, type.measure="class")
coef(lasso.out, s=lasso.out$lambda.min)

#Ridge MODEL  - doesn't suggest to take any variables out?
library(glmnet)
xx <- model.matrix(Target~., dat.tr)
yy <- as.integer(dat.tr$Target) #packages requires this to be numeric
ridge.out <- cv.glmnet(xx, yy, alpha=0, type.measure="class")
coef(ridge.out, s=ridge.out$lambda.min)

#Backward Selection - significant variables in equation below
#multinom(formula = Target ~ v18q1 + tamviv + CementorBetter + cielorazo + GasOrElectric + etecho1 + eviv1 + dis + hogar_mayor + edjefe + meaneduc + television + qmobilephone + area1 + depend, data = dat.tr)
mod.back <- step(mod.full,  direction="backward", trace=0)
test <- predict.glm(mod.back, type="response")
summary(mod.back)

#Forward Selection - significant variables in equation below
#multinom(formula = Target ~ rooms + v18q1 + tamviv + CementorBetter + cielorazo + GasOrElectric + elimbasu1 + epared1 + etecho1 + eviv1 + dis + comMSS + hogar_nin + hogar_adul + hogar_mayor + edjefe + edjefa + meaneduc + overcrowding + computer + television + qmobilephone + area1 + depend, data = dat.tr)
mod.back <- step(mod.full,  direction="forward", trace=0)
test <- predict.glm(mod.back, type="response")
summary(mod.back)

```



